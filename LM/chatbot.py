# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DzSHkzBEYjm-w7zQ6JEUGmyu5pYnCrpX

CHAT BOT Using Seq2Seq(Encoder and Decoder)
"""

# konlpy, mecab 등 설치
!pip3 install --upgrade pip
!pip3 install konlpy
!pip3 install tweepy==3.10.0

# 구글 드라이브 마운트
from google.colab import drive
drive.mount('/content/drive')

# 라이브러리 임포트
import pandas as pd
import matplotlib.pyplot as plt
import re
import numpy as np
import json
import tensorflow as tf
import os

#from konlpy.tag import Okt
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# TPU
!echo $COLAB_TPU_ADDR
TPU_PATH = f"grpc://{os.environ['COLAB_TPU_ADDR']}"

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_PATH)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

# GPU
!nvidia-smi
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')

print('Found GPU at: {}'.format(device_name))

tf.debugging.set_log_device_placement(True)
# 텐서 생성 - 작동확인
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print(c)

device_name = tf.test.gpu_device_name()

with tf.device(device_name):
  # something
  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
  c = tf.matmul(a, b)

# 경로 등
DRIVE_DATA_PATH = './drive/MyDrive/data_in/'

# DRIVE_VOCAB_PATH_WELLNESS = './drive/MyDrive/data_in/vacab_wellness.txt'
# TRAIN_INPUTS_WELLNESS = 'train_inputs_wellness.npy'
# TRAIN_OUTPUTS_WELLNESS = 'train_outputs_wellness.npy'
# TRAIN_TARGETS_WELLNESS = 'train_targets_wellness.npy'
# DATA_CONFIGS_WELLNESS = 'data_configs_wellness.json'
DRIVE_VOCAB_PATH_WELLNESS = './drive/MyDrive/data_in/vacab_.txt'
TRAIN_INPUTS_WELLNESS = 'train_inputs_.npy'
TRAIN_OUTPUTS_WELLNESS = 'train_outputs_.npy'
TRAIN_TARGETS_WELLNESS = 'train_targets_.npy'
DATA_CONFIGS_WELLNESS = 'data_configs_.json'

# DRIVE_VOCAB_PATH_CAHTBOT = './drive/MyDrive/data_in/vacab_chatbot.txt'
# TRAIN_INPUTS_CHATBOT = 'train_inputs_chatbot.npy'
# TRAIN_OUTPUTS_CHATBOT = 'train_outputs_chatbot.npy'
# TRAIN_TARGETS_CHATBOT = 'train_targets_chatbot.npy'
# DATA_CONFIGS_CHATBOT = 'data_configs_chatbot.json'
DRIVE_VOCAB_PATH_CAHTBOT = './drive/MyDrive/data_in/vacab.txt'
TRAIN_INPUTS_CHATBOT = 'train_inputs.npy'
TRAIN_OUTPUTS_CHATBOT = 'train_outputs.npy'
TRAIN_TARGETS_CHATBOT = 'train_targets.npy'
DATA_CONFIGS_CHATBOT = 'data_configs.json'


MODEL_NAME = 'seq2seq_kor'
BATCH_SIZE = 2
MAX_SEQUENCE = 20
EPOCH = 30
UNITS = 1024
EMBEDDING_DIM = 256
VALIDATION_SPLIT = 0.1 
DRIVE_RESULT_PATH = './drive/MyDrive/result/'
RESULT_PATH = './result/'

# 챗봇 데이터
data_chatbot = pd.read_csv(DRIVE_DATA_PATH + 'ChatbotData.csv', encoding='utf-8')

# 챗봇 데이터
data_wellness = pd.read_excel(DRIVE_DATA_PATH + '웰니스_대화_스크립트_데이터셋.xlsx')

data_chatbot.head(10)

data_chatbot.dropna(axis=0, inplace=True)
data_wellness.dropna(axis=0, inplace=True)

data_chatbot.describe()

question_chatbot, answer_chatbot = list(data_chatbot['Q']), list(data_chatbot['A'])

question_wellness, answer_wellness= list(data_wellness['유저']), list(data_wellness['챗봇'])

def data_preprocess(data):
    words=[]
    for x in data:
        x = re.sub('^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9'," ",str(x)).split()
        for y in x:
            words.append(y)
    return [word for word in words if word]

def morph(text):
    okt = Okt()
    result = []
    for x in text:
        temp = " ".join(okt.morphs(str(x).replace(' ', '')))
        result.append(temp)
    return result

def make_vocab(path, question, answer):
    """
    question, answer는 list
    """
    data=[]
    #question = morph(question)
    #answer = morph(answer)
    
    data.extend(question)
    data.extend(answer)
    data = data_preprocess(data)
    data = list(set(data))
    data[:0]=["<PAD>","<CLS>","<SEP>","<UNK>"]
    with open(path, 'w', encoding='UTF-8') as vocab:
        for x in data:
            vocab.write(x + '\n')
    return data

def open_vocab(path):
    result=[]
    with open(path,'r', encoding='utf-8') as vocab:
        for x in vocab:
            result.append(x.strip())
    return result
  
def vo(data):
    """
    make_vocab, open_vocab result
    """
    wordidx = {word: idx for idx, word in enumerate(data)}
    idxword = {idx: word for idx, word in enumerate(data)}
    vocab_size = len(wordidx)
    
    return wordidx, idxword, vocab_size

def encoding_data(text, vocab_dic, MAX_LEN = 20):
    input_data=[]
    texts_len=[]
    #text = morph(text)
    for x in text:
        x = re.sub('^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9',"",str(x)).split()
        temp=[]
        for y in x:
            if vocab_dic.get(y) is not None:
                temp.extend([vocab_dic[y]])
            else:
                temp.extend([vocab_dic['<UNK>']])
        
        if len(temp)> MAX_LEN:
            temp = temp[:MAX_LEN]
        
        texts_len.append(len(temp))
        temp = temp + (MAX_LEN-len(temp))*[vocab_dic['<PAD>']]
        
        input_data.append(temp)
        
    return np.asarray(input_data), texts_len

def decoding_input(text, vocab_dic, MAX_LEN = 20):
    input_data=[]
    texts_len=[]
    #text = morph(text)
    for x in text:
        x = re.sub('^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9',"",str(x)).split()
        temp=[]
        temp = [vocab_dic['<CLS>']]+[vocab_dic[y] if y in vocab_dic else vocab_dic['<UNK>'] for y in x]
        if len(temp)> MAX_LEN:
            temp = temp[:MAX_LEN]
        
        texts_len.append(len(temp))
        temp = temp + (MAX_LEN-len(temp))*[vocab_dic['<PAD>']]
        
        input_data.append(temp)
    return np.asarray(input_data), texts_len

def decoding_result(text, vocab_dic, MAX_LEN = 20):
    input_data=[]
    #text = morph(text)
    for x in text:
        x = re.sub('^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9',"",str(x)).split()
        temp=[]
        temp = [vocab_dic[y] if y in vocab_dic else vocab_dic['<UNK>'] for y in x]
        if len(temp)>= MAX_LEN:
            temp = temp[:MAX_LEN-1]+[vocab_dic['<SEP>']]
        else:
            temp += [vocab_dic['<SEP>']]
        
        temp += (MAX_LEN-len(temp))*[vocab_dic['<PAD>']]
        input_data.append(temp)
        
    return np.asarray(input_data)

def plot_graphs(history, metrics):
    plt.plot(history.history[metrics])
    plt.plot(history.history['val_'+metrics], '')
    plt.xlabel("Epochs")
    plt.ylabel(metrics)
    plt.legend([metrics, 'val_'+metrics])
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# chat_bot = make_vocab(DRIVE_VOCAB_PATH_CAHTBOT, question_chatbot, answer_chatbot)
# wordidx_chatbot, idxword_chatbot, vocab_size_chatbot = vo(chat_bot)
# input_data_chatbot, texts_len_chatbot = encoding_data(question_chatbot, wordidx_chatbot)
# output_data_chatbot, out_texts_len_chatbot = decoding_input(answer_chatbot, wordidx_chatbot)
# target_data_chatbot = decoding_result(answer_chatbot, wordidx_chatbot)
# data_configs_chatbot = {'wordidx':wordidx_chatbot, 'idxword':idxword_chatbot, 
#                         'vocab_size':vocab_size_chatbot, 'PAD': "<PAD>",
#                         'CLS': "<CLS>", 'SEP':"<SEP>", 'UNK':"<UNK>"}
# 
# np.save(open(DRIVE_DATA_PATH + TRAIN_INPUTS_CHATBOT, 'wb'), input_data_chatbot)
# np.save(open(DRIVE_DATA_PATH + TRAIN_OUTPUTS_CHATBOT, 'wb'), output_data_chatbot)
# np.save(open(DRIVE_DATA_PATH + TRAIN_TARGETS_CHATBOT, 'wb'), target_data_chatbot)
# json.dump(data_configs_chatbot, open(DRIVE_DATA_PATH + DATA_CONFIGS_CHATBOT, 'w'))
# 
# index_inputs_chatbot = np.load(open(DRIVE_DATA_PATH + TRAIN_INPUTS_CHATBOT, 'rb'))
# index_outputs_chatbot = np.load(open(DRIVE_DATA_PATH + TRAIN_OUTPUTS_CHATBOT , 'rb'))
# index_targets_chatbot = np.load(open(DRIVE_DATA_PATH + TRAIN_TARGETS_CHATBOT , 'rb'))
# prepro_configs_chatbot = json.load(open(DRIVE_DATA_PATH + DATA_CONFIGS_CHATBOT, 'r'))
# 
# wordidx_chatbot = prepro_configs_chatbot['wordidx']
# idxword_chatbot = prepro_configs_chatbot['idxword']
# std_index_chatbot = prepro_configs_chatbot['CLS']
# end_index_chatbot = prepro_configs_chatbot['SEP']
# vocab_size_chatbot = prepro_configs_chatbot['vocab_size']

# Commented out IPython magic to ensure Python compatibility.
# %%time
# wellness = make_vocab(DRIVE_VOCAB_PATH_WELLNESS, question_wellness, answer_wellness)
# wordidx_wellness, idxword_wellness, vocab_size_wellness = vo(wellness)
# input_data_wellness, texts_len_wellness = encoding_data(question_wellness, wordidx_wellness)
# output_data_wellness, out_texts_len_wellness = decoding_input(answer_wellness, wordidx_wellness)
# target_data_wellness = decoding_result(answer_wellness, wordidx_wellness)
# data_configs_wellness = {'wordidx':wordidx_wellness, 'idxword':idxword_wellness, 
#                         'vocab_size':vocab_size_wellness, 'PAD': "<PAD>",
#                         'CLS': "<CLS>", 'SEP':"<SEP>", 'UNK':"<UNK>"}
# 
# np.save(open(DRIVE_DATA_PATH + TRAIN_INPUTS_WELLNESS, 'wb'), input_data_wellness)
# np.save(open(DRIVE_DATA_PATH + TRAIN_OUTPUTS_WELLNESS, 'wb'), output_data_wellness)
# np.save(open(DRIVE_DATA_PATH + TRAIN_TARGETS_WELLNESS, 'wb'), target_data_wellness)
# json.dump(data_configs_wellness, open(DRIVE_DATA_PATH + DATA_CONFIGS_WELLNESS, 'w'))
# 
# index_inputs_wellness = np.load(open(DRIVE_DATA_PATH + TRAIN_INPUTS_WELLNESS, 'rb'))
# index_outputs_wellness = np.load(open(DRIVE_DATA_PATH + TRAIN_OUTPUTS_WELLNESS , 'rb'))
# index_targets_wellness = np.load(open(DRIVE_DATA_PATH + TRAIN_TARGETS_WELLNESS, 'rb'))
# prepro_configs_wellness = json.load(open(DRIVE_DATA_PATH + DATA_CONFIGS_WELLNESS, 'r'))
# 
# wordidx_wellness = prepro_configs_wellness['wordidx']
# idxword_wellness = prepro_configs_wellness['idxword']
# std_index_wellness = prepro_configs_wellness['CLS']
# end_index_wellness = prepro_configs_wellness['SEP']
# vocab_size_wellness = prepro_configs_wellness['vocab_size']

# wordidx = prepro_configs['wordidx']
# idxword = prepro_configs['idxword']
# std_index = prepro_configs['CLS']
# end_index = prepro_configs['SEP']
# vocab_size = prepro_configs['vocab_size']

class Encoder(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        self.batch_sz = batch_sz
        self.enc_units = enc_units
        self.vocab_size = vocab_size 
        self.embedding_dim = embedding_dim          
        
        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)
        self.gru = tf.keras.layers.GRU(self.enc_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform')

    def call(self, x, hidden):
        x = self.embedding(x)
        output, state = self.gru(x, initial_state = hidden)
        return output, state

    def initialize_hidden_state(self, inp):
        return tf.zeros((tf.shape(inp)[0], self.enc_units))

class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        hidden_with_time_axis = tf.expand_dims(query, 1)

        score = self.V(tf.nn.tanh(
            self.W1(values) + self.W2(hidden_with_time_axis)))

        attention_weights = tf.nn.softmax(score, axis=1)

        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

class Decoder(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
        super(Decoder, self).__init__()
        
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.vocab_size = vocab_size 
        self.embedding_dim = embedding_dim  
        
        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)
        self.gru = tf.keras.layers.GRU(self.dec_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform')
        self.fc = tf.keras.layers.Dense(self.vocab_size)

        self.attention = BahdanauAttention(self.dec_units)
        
    def call(self, x, hidden, enc_output):
        context_vector, attention_weights = self.attention(hidden, enc_output)

        x = self.embedding(x)

        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        output, state = self.gru(x)
        output = tf.reshape(output, (-1, output.shape[2]))
            
        x = self.fc(output)
        
        return x, state, attention_weights

class seq2seq(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_sz, wordidx, end_token_idx=2):    
        super(seq2seq, self).__init__()
        self.end_token_idx = end_token_idx
        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_sz) 
        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_sz) 
        self.wordidx = wordidx

    def call(self, x):
      
        inp, tar = x
        enc_hidden = self.encoder.initialize_hidden_state(inp)
        enc_output, enc_hidden = self.encoder(inp, enc_hidden)
        dec_hidden = enc_hidden
        predict_tokens = list()

        for t in range(0, tar.shape[1]):
            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:, t], 1), tf.float32) 
            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)
            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))   
        return tf.stack(predict_tokens, axis=1)
    
    def inference(self, x):
        inp  = x

        enc_hidden = self.encoder.initialize_hidden_state(inp)
        enc_output, enc_hidden = self.encoder(inp, enc_hidden)

        dec_hidden = enc_hidden
        
        dec_input = tf.expand_dims([self.wordidx["<CLS>"]], 1)
        
        predict_tokens = list()
        for t in range(0, MAX_SEQUENCE):
            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)
            predict_token = tf.argmax(predictions[0])
            
            if predict_token == self.end_token_idx:
                break
            
            predict_tokens.append(predict_token)
            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)   
            
        return tf.stack(predict_tokens, axis=0).numpy()

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')

def loss(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_mean(loss_)

def accuracy(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)
    pred *= mask    
    acc = train_accuracy(real, pred)
    return tf.reduce_mean(acc)

PATH = DRIVE_RESULT_PATH + MODEL_NAME
if not(os.path.isdir(PATH)):
        os.makedirs(os.path.join(PATH))

checkpoint_path_chatbot = PATH + '/weights_chatbot.h5'
cp_callback_chatbot = ModelCheckpoint(
    checkpoint_path_chatbot, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)

checkpoint_path_wellness = PATH + '/weights_wellness.h5'
cp_callback_wellness = ModelCheckpoint(
    checkpoint_path_wellness, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)

earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)

model_chatbot = seq2seq(vocab_size_chatbot, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, wordidx_chatbot, wordidx_chatbot["<SEP>"])
model_chatbot.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])
#model.run_eagerly = True

history_chatbot = model_chatbot.fit(
    [index_inputs_chatbot, index_outputs_chatbot], index_targets_chatbot,
    batch_size=BATCH_SIZE,epochs=EPOCH,validation_split=VALIDATION_SPLIT, 
    callbacks=[earlystop_callback, cp_callback_chatbot]
    )

model_wellness = seq2seq(vocab_size_wellness, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, wordidx_wellness, wordidx_wellness["<SEP>"])
model_wellness.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])
#model.run_eagerly = True

history_wellness = model_wellness.fit(
    [index_inputs_wellness, index_outputs_wellness], index_targets_wellness,
    batch_size=BATCH_SIZE, epochs=EPOCH,validation_split=VALIDATION_SPLIT, 
    callbacks=[earlystop_callback, cp_callback_wellness]
    )

model_chatbot.summary()
model_wellness.summary()

plot_graphs(history_chatbot, 'accuracy')
plot_graphs(history_chatbot, 'loss')

plot_graphs(history_wellness, 'accuracy')
plot_graphs(history_wellness, 'loss')

index_input_temp = index_inputs_chatbot[:2]
index_outputs_temp= index_outputs_chatbot[:2]
index_targets_temp= index_targets_chatbot[:2]

model_chatbot = seq2seq(vocab_size_chatbot, EMBEDDING_DIM, UNITS, UNITS, 
                        BATCH_SIZE, wordidx_chatbot, wordidx_chatbot["<SEP>"])
model_chatbot.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), 
                      metrics=[accuracy])
model_chatbot.fit(
    [index_input_temp, index_outputs_temp], index_targets_temp,
    batch_size=BATCH_SIZE, epochs=1,validation_split=VALIDATION_SPLIT
    )

#model_chatbot.load_weights(PATH + '/weights_chatbot.h5')
model_chatbot.load_weights(PATH + '/weights.h5')

ques = "sns 중독이 심해"
test_index_inputs, _ = encoding_data([ques], wordidx_chatbot)    
predict_tokens = model_chatbot.inference(test_index_inputs)
print(predict_tokens)
print(' '.join([idxword_chatbot[str(t)] for t in predict_tokens]))

index_input_temp = index_inputs_wellness[:2]
index_outputs_temp= index_outputs_wellness[:2]
index_targets_temp= index_targets_wellness[:2]

model_wellness = seq2seq(vocab_size_wellness, EMBEDDING_DIM, UNITS, UNITS, 
                         BATCH_SIZE, wordidx_wellness, wordidx_wellness["<SEP>"])
model_wellness.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), 
                       metrics=[accuracy])
model_wellness.fit(
    [index_input_temp, index_outputs_temp], index_targets_temp,
    batch_size=BATCH_SIZE, epochs=1,validation_split=VALIDATION_SPLIT
    )

#model_wellness.load_weights(PATH + '/weights_wellness.h5')
model_wellness.load_weights(PATH + '/weights_san.h5')

ques = "남편이 무서워"
test_index_inputs, _ = encoding_data([ques], wordidx_wellness)    
print(test_index_inputs)
predict_tokens = model_wellness.inference(test_index_inputs)
print(predict_tokens)
print(' '.join([idxword_wellness[str(t)] for t in predict_tokens]))